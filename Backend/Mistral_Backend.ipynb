{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Y1o4M3SlcHg4",
        "iLHtuzcIcK_A"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Installing and Importing all the necessary packages\n",
        "\n",
        "*Hey, We are Siddhant & Pavana and we are participating in Mistral FT Hackathon :3. To begin our Mistral Backend, we need to install and import the necessary libraries. This setup ensures we have all the tools required for data manipulation, visualization, and interaction with the Mistral API with FastAPI backend framework.*"
      ],
      "metadata": {
        "id": "do96K51zb6Fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper elevenlabs pyngrok fastapi[all] nest_asyncio mistralai -q\n",
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok\n",
        "!ngrok authtoken 2NqtkyBMF0KF99ofEU1fGz1pCJS_3XehxpoU47JGBiqY6tV7M"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_Y7T8KnmWm1C",
        "outputId": "8194e839-90c3-42cc-876f-eb9510a696ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/798.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/798.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m368.6/798.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m788.5/798.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.6/798.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.2/131.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "deb https://ngrok-agent.s3.amazonaws.com buster main\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 https://ngrok-agent.s3.amazonaws.com buster InRelease [20.3 kB]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:7 https://ngrok-agent.s3.amazonaws.com buster/main amd64 Packages [4,888 B]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,639 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,238 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,398 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [32.2 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,566 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,974 kB]\n",
            "Fetched 11.3 MB in 2s (5,329 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "46 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  ngrok\n",
            "0 upgraded, 1 newly installed, 0 to remove and 46 not upgraded.\n",
            "Need to get 6,507 kB of archives.\n",
            "After this operation, 0 B of additional disk space will be used.\n",
            "Get:1 https://ngrok-agent.s3.amazonaws.com buster/main amd64 ngrok amd64 3.12.0 [6,507 kB]\n",
            "Fetched 6,507 kB in 0s (27.6 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package ngrok.\n",
            "(Reading database ... 121925 files and directories currently installed.)\n",
            "Preparing to unpack .../ngrok_3.12.0_amd64.deb ...\n",
            "Unpacking ngrok (3.12.0) ...\n",
            "Setting up ngrok (3.12.0) ...\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "from fastapi import FastAPI, UploadFile, File, HTTPException, Body\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import locale\n",
        "from pydantic import BaseModel\n",
        "from elevenlabs.client import ElevenLabs\n",
        "import uuid\n",
        "import os\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "from fastapi.responses import FileResponse\n",
        "import gc\n",
        "import torch\n",
        "from elevenlabs import save\n",
        "from mistralai.models.chat_completion import ChatMessage\n",
        "from mistralai.client import MistralClient\n",
        "from google.colab import userdata\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "client = ElevenLabs(api_key = userdata.get('ELEVENLABS_API_KEY'))\n",
        "mistralclient = MistralClient(api_key=userdata.get('MISTRAL_API_KEY'))\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
        "voice_model = whisper.load_model(\"large-v3\",device=device)\n",
        "port=8888\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAHmghQhW5_F",
        "outputId": "56e4693d-7754-4972-ac27-bab8faa9c393"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████████████████████████████████| 2.88G/2.88G [00:29<00:00, 106MiB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Datamodels and pre-prompts!\n",
        "\n",
        "*To streamline the process of generating responses, we define specific data models and pre-prompts that structure the input and guide the model's behavior.*"
      ],
      "metadata": {
        "id": "Y1o4M3SlcHg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TTSNER(BaseModel):\n",
        "    text: str\n",
        "    emotion: str = \"Cheerful & Professional\""
      ],
      "metadata": {
        "id": "z4I81SzQXgz0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprompt = \"\"\"You are an intelligent clinical language model.\n",
        "Below is a snippet of patient's electronic health record note and a following instruction with question from healthcare professional.\n",
        "Write a response that appropriately completes the instruction.\n",
        "The response should provide the accurate answer to the instruction, while being concise.\"\"\"\n",
        "\n",
        "print(preprompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akrYwoWaZ8Jv",
        "outputId": "225409fd-73ca-4bd3-f772-91cfd950f1cd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an intelligent clinical language model.\n",
            "Below is a snippet of patient's electronic health record note and a following instruction with question from healthcare professional.\n",
            "Write a response that appropriately completes the instruction.\n",
            "The response should provide the accurate answer to the instruction, while being concise.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running the Backend and fetching the NGROK Link\n",
        "\n",
        "*To deploy and access the backend of our Mistral FT Hacks project, we utilize FastAPI to create an API server. This server handles various endpoints for tasks such as transcribing audio, performing text-to-speech (TTS) conversions, and interacting with the fine-tuned Mistral model.*"
      ],
      "metadata": {
        "id": "iLHtuzcIcK_A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M22BtEsWWpe",
        "outputId": "718ae0c5-9e0c-4abd-91bb-c5b954c0cf16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://523e-34-19-19-170.ngrok-free.app\n",
            "INFO:     Started server process [293]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8888 (Press CTRL+C to quit)\n",
            "INFO:     2401:4900:1c97:8f58:b643:dd0f:6595:1114:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c97:8f58:b643:dd0f:6595:1114:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c97:8f58:b643:dd0f:6595:1114:0 - \"OPTIONS /mistral/ HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c97:8f58:b643:dd0f:6595:1114:0 - \"POST /mistral/ HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c97:8f58:b643:dd0f:6595:1114:0 - \"OPTIONS /transcribe/ HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c97:8f58:b643:dd0f:6595:1114:0 - \"POST /transcribe/ HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c97:8f58:b643:dd0f:6595:1114:0 - \"POST /mistral/ HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c97:8f58:b643:dd0f:6595:1114:0 - \"OPTIONS /labs-tts HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c97:8f58:b643:dd0f:6595:1114:0 - \"POST /labs-tts HTTP/1.1\" 307 Temporary Redirect\n",
            "INFO:     2401:4900:1c97:8f58:b643:dd0f:6595:1114:0 - \"OPTIONS /labs-tts/ HTTP/1.1\" 200 OK\n",
            "INFO:     2401:4900:1c97:8f58:b643:dd0f:6595:1114:0 - \"POST /labs-tts/ HTTP/1.1\" 200 OK\n",
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [293]\n"
          ]
        }
      ],
      "source": [
        "app = FastAPI(title=\"Mistral FT Hacks Backend\")\n",
        "\n",
        "app.add_middleware(\n",
        "CORSMiddleware,\n",
        "allow_origins=[\"*\"],\n",
        "allow_credentials=True,\n",
        "allow_methods=[\"*\"],\n",
        "allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def home():\n",
        "    return \"Mistral FT Hack\"\n",
        "\n",
        "@app.post(\"/transcribe/\")\n",
        "async def transcribe_audio(file: UploadFile = File(...)):#,token: str = Header(...)):\n",
        "    try:\n",
        "      #decoded_token = decode_token(token)\n",
        "      audio_path = f\"{uuid.uuid4()}.webm\"\n",
        "      with open(audio_path, \"wb\") as f:\n",
        "          f.write(await file.read())\n",
        "      result = voice_model.transcribe(whisper.pad_or_trim(whisper.load_audio(audio_path)))\n",
        "      text = result[\"text\"]\n",
        "      src_lang = result[\"language\"]\n",
        "      os.remove(audio_path)\n",
        "      return {\"text\": text,\"src_lang\":src_lang}\n",
        "    except HTTPException as e:\n",
        "        raise e\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/labs-tts/\")\n",
        "async def labs_tts(request: TTSNER = Body(...)):#,token: str = Header(...)):\n",
        "    try:\n",
        "      #decoded_token = decode_token(token)\n",
        "      out = f\"{uuid.uuid4()}.ogg\"\n",
        "      async def remove():\n",
        "          loop = asyncio.get_event_loop()\n",
        "          await loop.run_in_executor(None, lambda: os.remove(out))\n",
        "      audio = client.generate(\n",
        "        text=request.text,\n",
        "        voice=\"Paul\",\n",
        "        model=\"eleven_multilingual_v2\"\n",
        "      )\n",
        "      save(audio,out)\n",
        "      return FileResponse(out,headers={\"Content-Disposition\":f\"attachment; filename={out}\"},background=remove)\n",
        "    except HTTPException as e:\n",
        "        raise e\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.post(\"/mistral/\")\n",
        "async def mistral(request: TTSNER = Body(...)):#,token: str = Header(...)):\n",
        "    try:\n",
        "      #decoded_token = decode_token(token)\n",
        "      result = mistralclient.chat(model='ft:open-mistral-7b:885d8e28:20240630:ddcad168',\n",
        "                                  messages=[ChatMessage(role='user',\n",
        "                                  content=preprompt+'\\n\\n'+request.text)]).dict()[\"choices\"][0][\"message\"][\"content\"]\n",
        "      return {\"text\":result}\n",
        "    except HTTPException as e:\n",
        "        raise e\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    nest_asyncio.apply()\n",
        "    ngrok_tunnel = ngrok.connect(port)\n",
        "    print('Public URL:', ngrok_tunnel.public_url)\n",
        "    uvicorn.run(app,port=port)"
      ]
    }
  ]
}