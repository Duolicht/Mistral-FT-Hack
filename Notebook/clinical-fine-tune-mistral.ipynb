{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q accelerate peft==0.4.0 bitsandbytes transformers==4.34.1 trl==0.4.7\n!pip install -U datasets\n\nimport os\nimport torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import LoraConfig, PeftModel\nfrom trl import SFTTrainer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-21T15:16:04.180816Z","iopub.execute_input":"2024-01-21T15:16:04.181245Z","iopub.status.idle":"2024-01-21T15:17:13.277877Z","shell.execute_reply.started":"2024-01-21T15:16:04.181208Z","shell.execute_reply":"2024-01-21T15:17:13.276735Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nCollecting datasets\n  Obtaining dependency information for datasets from https://files.pythonhosted.org/packages/ec/93/454ada0d1b289a0f4a86ac88dbdeab54921becabac45da3da787d136628f/datasets-2.16.1-py3-none-any.whl.metadata\n  Downloading datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.12.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nCollecting pyarrow-hotfix (from datasets)\n  Obtaining dependency information for pyarrow-hotfix from https://files.pythonhosted.org/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl.metadata\n  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nCollecting fsspec[http]<=2023.10.0,>=2023.1.0 (from datasets)\n  Obtaining dependency information for fsspec[http]<=2023.10.0,>=2023.1.0 from https://files.pythonhosted.org/packages/e8/f6/3eccfb530aac90ad1301c582da228e4763f19e719ac8200752a4841b0b2d/fsspec-2023.10.0-py3-none-any.whl.metadata\n  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nCollecting huggingface-hub>=0.19.4 (from datasets)\n  Obtaining dependency information for huggingface-hub>=0.19.4 from https://files.pythonhosted.org/packages/3d/0a/aed3253a9ce63d9c90829b1d36bc44ad966499ff4f5827309099c8c9184b/huggingface_hub-0.20.2-py3-none-any.whl.metadata\n  Downloading huggingface_hub-0.20.2-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nDownloading datasets-2.16.1-py3-none-any.whl (507 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hDownloading huggingface_hub-0.20.2-py3-none-any.whl (330 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.3/330.3 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\nDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow-hotfix, fsspec, huggingface-hub, datasets\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2023.12.2\n    Uninstalling fsspec-2023.12.2:\n      Successfully uninstalled fsspec-2023.12.2\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.17.3\n    Uninstalling huggingface-hub-0.17.3:\n      Successfully uninstalled huggingface-hub-0.17.3\n  Attempting uninstall: datasets\n    Found existing installation: datasets 2.1.0\n    Uninstalling datasets-2.1.0:\n      Successfully uninstalled datasets-2.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ngcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.10.0 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.12.1 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.12.1 which is incompatible.\ns3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\ntokenizers 0.14.1 requires huggingface_hub<0.18,>=0.16.4, but you have huggingface-hub 0.20.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed datasets-2.16.1 fsspec-2023.10.0 huggingface-hub-0.20.2 pyarrow-hotfix-0.6\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"# The model that you want to train from the Hugging Face hub\n#model_name = \"mistralai/Mistral-7B-v0.1\"\nmodel_name = \"filipealmeida/Mistral-7B-Instruct-v0.1-sharded\"\n\n# The instruction dataset to use\ndataset_name = \"starmpcc/Asclepius-Synthetic-Clinical-Notes\"\n\n# Fine-tuned model name\nnew_model = \"clinical-ehr-prototype-0.1\"","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:17:13.279963Z","iopub.execute_input":"2024-01-21T15:17:13.280653Z","iopub.status.idle":"2024-01-21T15:17:13.289725Z","shell.execute_reply.started":"2024-01-21T15:17:13.280621Z","shell.execute_reply":"2024-01-21T15:17:13.288330Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"################################################################################\n# QLoRA parameters\n################################################################################\n\n# LoRA attention dimension\nlora_r = 64\n\n# Alpha parameter for LoRA scaling\nlora_alpha = 16\n\n# Dropout probability for LoRA layers\nlora_dropout = 0.1","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:17:13.291937Z","iopub.execute_input":"2024-01-21T15:17:13.292341Z","iopub.status.idle":"2024-01-21T15:17:13.303859Z","shell.execute_reply.started":"2024-01-21T15:17:13.292281Z","shell.execute_reply":"2024-01-21T15:17:13.302950Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"################################################################################\n# bitsandbytes parameters\n################################################################################\n\n# Activate 4-bit precision base model loading\nuse_4bit = True\n\n# Compute dtype for 4-bit base models\nbnb_4bit_compute_dtype = \"float16\"\n\n# Quantization type (fp4 or nf4)\nbnb_4bit_quant_type = \"nf4\"\n\n# Activate nested quantization for 4-bit base models (double quantization)\nuse_nested_quant = False","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:17:13.306764Z","iopub.execute_input":"2024-01-21T15:17:13.307159Z","iopub.status.idle":"2024-01-21T15:17:13.314232Z","shell.execute_reply.started":"2024-01-21T15:17:13.307129Z","shell.execute_reply":"2024-01-21T15:17:13.313219Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"################################################################################\n# TrainingArguments parameters\n################################################################################\n\n# Output directory where the model predictions and checkpoints will be stored\noutput_dir = \"./results\"\n\n# Number of training epochs\nnum_train_epochs = 1\n\n# Enable fp16/bf16 training (set bf16 to True with an A100)\nfp16 = False\nbf16 = False\n\n# Batch size per GPU for training\nper_device_train_batch_size = 4\n\n# Batch size per GPU for evaluation\nper_device_eval_batch_size = 4\n\n# Number of update steps to accumulate the gradients for\ngradient_accumulation_steps = 1\n\n# Enable gradient checkpointing\ngradient_checkpointing = True\n\n# Maximum gradient normal (gradient clipping)\nmax_grad_norm = 0.3\n\n# Initial learning rate (AdamW optimizer)\nlearning_rate = 2e-4\n\n# Weight decay to apply to all layers except bias/LayerNorm weights\nweight_decay = 0.001\n\n# Optimizer to use\noptim = \"paged_adamw_32bit\"\n\n# Learning rate schedule\nlr_scheduler_type = \"cosine\"\n\n# Number of training steps (overrides num_train_epochs)\nmax_steps = -1\n\n# Ratio of steps for a linear warmup (from 0 to learning rate)\nwarmup_ratio = 0.03\n\n# Group sequences into batches with same length\n# Saves memory and speeds up training considerably\ngroup_by_length = True\n\n# Save checkpoint every X updates steps\nsave_steps = 0\n\n# Log every X updates steps\nlogging_steps = 25","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:17:13.315616Z","iopub.execute_input":"2024-01-21T15:17:13.316290Z","iopub.status.idle":"2024-01-21T15:17:13.325852Z","shell.execute_reply.started":"2024-01-21T15:17:13.316253Z","shell.execute_reply":"2024-01-21T15:17:13.324920Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"################################################################################\n# SFT parameters\n################################################################################\n\n# Maximum sequence length to use\nmax_seq_length = None\n\n# Pack multiple short examples in the same input sequence to increase efficiency\npacking = False","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:17:13.327287Z","iopub.execute_input":"2024-01-21T15:17:13.327913Z","iopub.status.idle":"2024-01-21T15:17:13.344623Z","shell.execute_reply.started":"2024-01-21T15:17:13.327877Z","shell.execute_reply":"2024-01-21T15:17:13.343512Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"alpaca_prompt = \"\"\"You are an intelligent clinical languge model.\nBelow is a snippet of patient's electronic health record note and a following instruction with question from healthcare professional.\nWrite a response that appropriately completes the instruction.\nThe response should provide the accurate answer to the instruction, while being concise.\n\n### Instruction:\n{}\n\n### Patient's Electronic Health Record Note:\n{}\n\n### Question:\n{}\n\n### Response:\n{}\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:17:13.345949Z","iopub.execute_input":"2024-01-21T15:17:13.346303Z","iopub.status.idle":"2024-01-21T15:17:13.354941Z","shell.execute_reply.started":"2024-01-21T15:17:13.346273Z","shell.execute_reply":"2024-01-21T15:17:13.353957Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def formatting_prompts_func(examples):\n    instructions = examples[\"task\"]\n    inputs       = examples[\"question\"]\n    outputs      = examples[\"answer\"]\n    context      = examples[\"note\"]\n    return {\"text\": [alpaca_prompt.format(instruction, note, input, output) for instruction, note, input, output in zip(instructions, context, inputs, outputs)]}","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:17:13.356146Z","iopub.execute_input":"2024-01-21T15:17:13.356463Z","iopub.status.idle":"2024-01-21T15:17:13.368103Z","shell.execute_reply.started":"2024-01-21T15:17:13.356438Z","shell.execute_reply":"2024-01-21T15:17:13.367232Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Load dataset (you can process it here)\n#dataset = load_dataset(dataset_name, split=\"train\")\ndataset = load_dataset(dataset_name, split='train[:1200]')\ndataset = dataset.map(formatting_prompts_func, batched = True)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:17:13.369196Z","iopub.execute_input":"2024-01-21T15:17:13.369561Z","iopub.status.idle":"2024-01-21T15:17:33.993183Z","shell.execute_reply.started":"2024-01-21T15:17:13.369534Z","shell.execute_reply":"2024-01-21T15:17:33.992149Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/2.91k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"437d972e96c14abdabb1d673431c61fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/402M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55e6a2adc6014eedb88dad44e4f9ee32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9fdad73db194d0faeeea743a16d6b7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8dbe1c375c2491592019c773dea3e3c"}},"metadata":{}}]},{"cell_type":"code","source":"# Load tokenizer and model with QLoRA configuration\ncompute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=use_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Check GPU compatibility with bfloat16\nif compute_dtype == torch.float16 and use_4bit:\n    major, _ = torch.cuda.get_device_capability()\n    if major >= 8:\n        print(\"=\" * 80)\n        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n        print(\"=\" * 80)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:17:33.996009Z","iopub.execute_input":"2024-01-21T15:17:33.996330Z","iopub.status.idle":"2024-01-21T15:17:34.003945Z","shell.execute_reply.started":"2024-01-21T15:17:33.996303Z","shell.execute_reply":"2024-01-21T15:17:34.003056Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map=\"auto\"\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:17:34.005207Z","iopub.execute_input":"2024-01-21T15:17:34.005632Z","iopub.status.idle":"2024-01-21T15:19:15.137860Z","shell.execute_reply.started":"2024-01-21T15:17:34.005597Z","shell.execute_reply":"2024-01-21T15:19:15.136975Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38e84219b3ad4fae93272399233de3ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79fade80f7794a37bf247c348ae60b04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fa415f0f9bb4179adad4dc85d9fe556"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00008.bin:   0%|          | 0.00/1.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e30baca6b7fe4150a261305691bd5057"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00008.bin:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d8e9e33d09a4c57bedf8767e125d0ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00003-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"329edcb1397041cfbeb2973ee433e27d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00004-of-00008.bin:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0ef66f869fc049d8a250305af3d9d1e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00005-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76fa909f16fc40319c48f891dc5f02e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00006-of-00008.bin:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91dd67d464804a71a40aa0eb9fcfbd3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00007-of-00008.bin:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c75bc39cef44f1cacea93bf201d248f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00008-of-00008.bin:   0%|          | 0.00/816M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e990c321deb451c9208cabc44e0527a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80200f21ffef417d98948c207b8162b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45b47c0aef18405cb680d0bedb4bec3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d3ba30290d8490fab34784d04155e8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10d2bd60f4b44fde8ca34b531ac35425"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f3f08b53f2941cea49dc95c497475e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dbbe4845fbf406b8ab96691acbd81ab"}},"metadata":{}}]},{"cell_type":"code","source":"# Load LoRA configuration\npeft_config = LoraConfig(\n        r=32,\n        lora_alpha=64,\n        target_modules=[\n            \"q_proj\",\n            \"k_proj\",\n            \"v_proj\",\n            \"o_proj\",\n            \"gate_proj\",\n            \"up_proj\",\n            \"down_proj\",\n            \"lm_head\",\n        ],\n        bias=\"none\",\n        lora_dropout=0.05,\n        task_type=\"CAUSAL_LM\",\n    )\n\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\"\n)\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:19:15.139313Z","iopub.execute_input":"2024-01-21T15:19:15.139942Z","iopub.status.idle":"2024-01-21T15:20:21.790355Z","shell.execute_reply.started":"2024-01-21T15:19:15.139906Z","shell.execute_reply":"2024-01-21T15:20:21.789570Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3aa2ae17914496e89bef0a0c8bfa33e"}},"metadata":{}}]},{"cell_type":"code","source":"#start the training\ntrainer.train()\ntrainer.model.save_pretrained(new_model)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T15:20:21.791451Z","iopub.execute_input":"2024-01-21T15:20:21.791737Z","iopub.status.idle":"2024-01-21T18:33:57.413772Z","shell.execute_reply.started":"2024-01-21T15:20:21.791711Z","shell.execute_reply":"2024-01-21T18:33:57.412673Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [300/300 3:12:42, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.174000</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.026900</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.069400</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.017100</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.047800</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.988600</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.015700</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.932000</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.026000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.969000</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.972800</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.945700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"# Ignore warnings\nlogging.set_verbosity(logging.CRITICAL)\n\n# Run text generation pipeline with our next model\nprompt = \"\"\"\nYou are an intelligent clinical languge model.\nBelow is a snippet of patient's electronic health record note and a following instruction with question from healthcare professional.\nWrite a response that appropriately completes the instruction.\nThe response should provide the accurate answer to the instruction, while being concise.\n\n### Instruction:\nParaphrasing\n\n### Patient's Electronic Health Record Note:\nDischarge Summary:\n\nPatient: 60-year-old male with moderate ARDS from COVID-19\n\nHospital Course:\n\nThe patient was admitted to the hospital with symptoms of fever, dry cough, and dyspnea. During physical therapy on the acute ward, the patient experienced coughing attacks that induced oxygen desaturation and dyspnea with any change of position or deep breathing. To avoid rapid deterioration and respiratory failure, a step-by-step approach was used for position changes. The breathing exercises were adapted to avoid prolonged coughing and oxygen desaturation, and with close monitoring, the patient managed to perform strength and walking exercises at a low level. Exercise progression was low initially but increased daily until hospital discharge to a rehabilitation clinic on day 10.\n\nClinical Outcome:\n\nThe patient was discharged on day 10 to a rehabilitation clinic making satisfactory progress with all symptoms resolved.\n\nFollow-up:\n\nThe patient will receive follow-up care at the rehabilitation clinic, with regular monitoring of progress and further rehabilitation exercises until full recovery. Any new symptoms or concerns should be reported to the clinic immediately.\n\nOverall Impression:\n\nThe patient responded well to treatment, and with appropriate medical intervention, was able to overcome the difficulties faced during hospitalization for ARDS from COVID-19. The patient's level of care was of a high standard, with all necessary therapy provided and monitoring of progress before discharge.\n\n### Question:\nCan you provide a simplified paraphrase of the sentence, 'To avoid rapid deterioration and respiratory failure, a step-by-step approach was used for position changes' in the patient's discharge summary?\n\"\"\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\nresult = pipe(f\"[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2024-01-21T18:33:57.415470Z","iopub.execute_input":"2024-01-21T18:33:57.416303Z","iopub.status.idle":"2024-01-21T18:34:01.273283Z","shell.execute_reply.started":"2024-01-21T18:33:57.416264Z","shell.execute_reply":"2024-01-21T18:34:01.272238Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1421: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1260: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1268: UserWarning: Input length of input_ids is 484, but `max_length` is set to 20. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n","output_type":"stream"},{"name":"stdout","text":"[INST] \nYou are an intelligent clinical languge model.\nBelow is a snippet of patient's electronic health record note and a following instruction with question from healthcare professional.\nWrite a response that appropriately completes the instruction.\nThe response should provide the accurate answer to the instruction, while being concise.\n\n### Instruction:\nParaphrasing\n\n### Patient's Electronic Health Record Note:\nDischarge Summary:\n\nPatient: 60-year-old male with moderate ARDS from COVID-19\n\nHospital Course:\n\nThe patient was admitted to the hospital with symptoms of fever, dry cough, and dyspnea. During physical therapy on the acute ward, the patient experienced coughing attacks that induced oxygen desaturation and dyspnea with any change of position or deep breathing. To avoid rapid deterioration and respiratory failure, a step-by-step approach was used for position changes. The breathing exercises were adapted to avoid prolonged coughing and oxygen desaturation, and with close monitoring, the patient managed to perform strength and walking exercises at a low level. Exercise progression was low initially but increased daily until hospital discharge to a rehabilitation clinic on day 10.\n\nClinical Outcome:\n\nThe patient was discharged on day 10 to a rehabilitation clinic making satisfactory progress with all symptoms resolved.\n\nFollow-up:\n\nThe patient will receive follow-up care at the rehabilitation clinic, with regular monitoring of progress and further rehabilitation exercises until full recovery. Any new symptoms or concerns should be reported to the clinic immediately.\n\nOverall Impression:\n\nThe patient responded well to treatment, and with appropriate medical intervention, was able to overcome the difficulties faced during hospitalization for ARDS from COVID-19. The patient's level of care was of a high standard, with all necessary therapy provided and monitoring of progress before discharge.\n\n### Question:\nCan you provide a simplified paraphrase of the sentence, 'To avoid rapid deterioration and respiratory failure, a step-by-step approach was used for position changes' in the patient's discharge summary?\n [/INST] To\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Empty VRAM\nimport gc\ndel model\ndel pipe\ndel trainer\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-21T18:34:01.274825Z","iopub.execute_input":"2024-01-21T18:34:01.275480Z","iopub.status.idle":"2024-01-21T18:34:01.613939Z","shell.execute_reply.started":"2024-01-21T18:34:01.275441Z","shell.execute_reply":"2024-01-21T18:34:01.612838Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"39638"},"metadata":{}}]},{"cell_type":"code","source":"# Reload model in FP16 and merge it with LoRA weights\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)\nmodel = PeftModel.from_pretrained(base_model, new_model)\nmodel = model.merge_and_unload()\n\n# Reload tokenizer to save it\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"execution":{"iopub.status.busy":"2024-01-21T18:34:01.615386Z","iopub.execute_input":"2024-01-21T18:34:01.616321Z","iopub.status.idle":"2024-01-21T18:37:22.545289Z","shell.execute_reply.started":"2024-01-21T18:34:01.616282Z","shell.execute_reply":"2024-01-21T18:37:22.543982Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31e6f418dcbd40a39d3c7b7c43af7e8e"}},"metadata":{}}]},{"cell_type":"code","source":"prompt = \"\"\"\nYou are an intelligent clinical languge model.\nBelow is a snippet of patient's electronic health record note and a following instruction with question from healthcare professional.\nWrite a response that appropriately completes the instruction.\nThe response should provide the accurate answer to the instruction, while being concise.\n\n### Instruction:\nAbbreviation Expansion\n\n### Patient's Electronic Health Record Note:\nHospital Course: \n\nThis 66-year-old male patient was admitted due to an ischemic left-hemispheric stroke in addition to a dry cough and fever. The patient tested positive for SARS-CoV-2 and experienced severe ARDS, resulting in intubation and ICU admission. The patient underwent veno-venous extracorporeal membrane oxygenation and physical therapy was initiated to focus on perception training, movement exercises, airway-clearing techniques, dysphagia therapy, and mobilization. Despite a trial of sedation cessation, the patient remained somnolent and unable to communicate or follow commands. A side-edge positioning was initiated in combination with intensive exercise training including trunk and head control. Muscle tone and strength remained severely reduced, particularly on his hemiplegic side, and a second SOEB trial failed. Occupational therapy was involved to support functional initiation of upper limb movements and to integrate perception-training into activities of daily living. Currently, the patient remains functionally dependent, tolerates spontaneous breathing trials, and is alert during therapy, although he cannot communicate. He is considered stable and functionally dependent (CPAx 6/50).\n\n### Question:\nWhat are the abbreviated terms in the given discharge summary that require expansion?\n\"\"\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\nresult = pipe(f\"[INST] {prompt} [/INST]\",max_length=584)[0]['generated_text']\nstart_index = result.find(\"[/INST]\") + len(\"[/INST]\")\nend_index = result.find(\"'\", start_index)\nresponse = result[start_index:end_index]\nprint(response)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T18:54:52.200523Z","iopub.execute_input":"2024-01-21T18:54:52.200866Z","iopub.status.idle":"2024-01-21T18:55:07.591796Z","shell.execute_reply.started":"2024-01-21T18:54:52.200839Z","shell.execute_reply":"2024-01-21T18:55:07.590814Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":" The abbreviated terms in the given discharge summary that require expansion are SARS-CoV-2, ARDS, ICU, SOEB, CPAx, and CPAx 6/50.\n\n### Response:\nThe abbreviated terms in the given discharge summary that require expansion are SARS-CoV-2, ARDS, ICU, SOEB, CPAx, and CPAx 6/50. SARS-CoV-2 stands for severe acute respiratory syndrome coronavirus 2, ARDS stands for acute respiratory distress syndrome, ICU stands for intensive care unit, SOEB stands for spontaneous breathing exercise, CPAx stands for Canadian Physical Activity Assessment, and CPAx 6/50 stands for a score of 6 out of 50 on the Canadian Physical Activity Assessment\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-01-21T18:37:23.150517Z","iopub.execute_input":"2024-01-21T18:37:23.150853Z","iopub.status.idle":"2024-01-21T18:37:23.178936Z","shell.execute_reply.started":"2024-01-21T18:37:23.150824Z","shell.execute_reply":"2024-01-21T18:37:23.177905Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2beb5fd37a6f4cfa9ba67a6632750237"}},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub(new_model, use_temp_dir=False)\ntokenizer.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-21T18:41:55.999143Z","iopub.execute_input":"2024-01-21T18:41:55.999527Z","iopub.status.idle":"2024-01-21T18:47:29.756764Z","shell.execute_reply.started":"2024-01-21T18:41:55.999498Z","shell.execute_reply":"2024-01-21T18:47:29.755700Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75a1ff97c67b4aa295922f512fe0cd29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00002.bin:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1ae4c00076b418b8b2111d48b058214"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a64e3f33e1ea455a9343e652a08f8d6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6c1a475b022421d8d67a387f820cc88"}},"metadata":{}},{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/elucidator8918/clinical-ehr-prototype-0.1/commit/a80aab57798048f5bb6c21ae6ae50b229a13a3bc', commit_message='Upload tokenizer', commit_description='', oid='a80aab57798048f5bb6c21ae6ae50b229a13a3bc', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}